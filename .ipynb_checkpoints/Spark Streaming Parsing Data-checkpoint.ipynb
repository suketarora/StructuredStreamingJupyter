{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Parsing Data\n",
    "<!-- Most manipulations we do will involve structuring data.  We demonstrate how to use case classes and Scala Reflection to easily structure our data and account for missing or incomplete fields. -->\n",
    "\n",
    "Much as with datasets, we can use a `case class` to represent rows of data.  The case class's attributes correspond to the json field names or (as in this case) the CSV column names.\n",
    "\n",
    "However, unlike with datasets, we cannot ask the reader to infer the schema.  Instead, we will use `ScalaReflection` to generate a schema for our case class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,city,country,age\n",
      "Amy,Paris,FR,30\n",
      "Bob,New York,US,22\n",
      "Charlie,London,UK,35\n",
      "Denise,San Francisco,US,22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"cat data/people/1.csv\" ! // run bash command using bang after a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Person(\n",
    "    name: String,\n",
    "    city: String,\n",
    "    country: String,\n",
    "    age: Option[Int]\n",
    ") extends Serializable {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caseSchema = StructType(StructField(name,StringType,true), StructField(city,StringType,true), StructField(country,StringType,true), StructField(age,IntegerType,true))\n",
       "peopleStream = [name: string, city: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@407054a6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+------+-------+----+\n",
      "|   name|  city|country| age|\n",
      "+-------+------+-------+----+\n",
      "| Edward|London|     UK|  53|\n",
      "|Francis|  null|     FR|  22|\n",
      "| George|London|     UK|null|\n",
      "+-------+------+-------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+-------------+-------+---+\n",
      "|   name|         city|country|age|\n",
      "+-------+-------------+-------+---+\n",
      "|    Amy|        Paris|     FR| 30|\n",
      "|    Bob|     New York|     US| 22|\n",
      "|Charlie|       London|     UK| 35|\n",
      "| Denise|San Francisco|     US| 22|\n",
      "+-------+-------------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "\n",
    "// create schema for parsing data\n",
    "val caseSchema = (ScalaReflection\n",
    "    .schemaFor[Person]\n",
    "    .dataType\n",
    "    .asInstanceOf[StructType])\n",
    "\n",
    "val peopleStream = (spark.readStream\n",
    "    .schema(caseSchema)\n",
    "    .option(\"header\", true)  // Headers are matched to Person properties\n",
    "    .option(\"maxFilesPerTrigger\", 1)  // each file is read in a separate batch\n",
    "    .csv(\"data/people/\")  // load a CSV file\n",
    "    .as[Person])\n",
    "  \n",
    "(peopleStream.writeStream\n",
    "    .outputMode(\"append\")  // write results to screen\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Columns in Structured Streaming\n",
    "<!-- Structured Streaming makes heavy use of Column objects for manipulating data.  In this section, we explain various ways in which the Column objects can be constructed from columns in our structured stream or by combining other columns. -->\n",
    "\n",
    "Datasets use a dataframe syntax to refer to columns (which are themselves `Column` objects).  There are a number of ways to do this:\n",
    "- `peopleStream(\"country\")`\n",
    "- `peopleStream.col(\"country\")`\n",
    "- `$\"country\"`\n",
    "- `'country`\n",
    "\n",
    "The first two are more explicit as they tell Spark which data stream to use.  This is useful in joins when we want to specify the table more explicitly.  The second two are more implicit as they do not specify the data stream.  These are more useful for single datastream operations.  The symbols need to be imported from `spark.implicits`.\n",
    "\n",
    "There are actually multiple ways to construct columns:\n",
    "- The above allows us to reference `Column`s already in a dataframe.\n",
    "- We can also construct a `Column` from other `Column`s using binary operators like `===` (equality), `>`, `<=`, `.plus`, `-`, `.startsWith`, or `&&`, depending on the underlying value of the column.\n",
    "- Finally, we can rename the columns (keeping the values) with the operator `as`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Filtering Columns Using Structured Streaming\n",
    "<!-- We'll demonstrate how to select and filter columns using Structured Streaming. -->\n",
    "\n",
    "We'll demonstrate these using the `select` method, which takes any non-zero number of `Column` arguments and returns a dataframe with those arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@50a14e"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+--------+---------+\n",
      "|in_UK|under_30|U_Country|\n",
      "+-----+--------+---------+\n",
      "| true|   false|     true|\n",
      "|false|    true|    false|\n",
      "| true|    null|     true|\n",
      "+-----+--------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+--------+---------+\n",
      "|in_UK|under_30|U_Country|\n",
      "+-----+--------+---------+\n",
      "|false|    true|    false|\n",
      "|false|    true|     true|\n",
      "| true|   false|     true|\n",
      "|false|    true|     true|\n",
      "+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(peopleStream.select(\n",
    "    $\"country\" === \"UK\" as \"in_UK\",\n",
    "    $\"age\" <= 30 as \"under_30\",\n",
    "    'country startsWith \"U\" as \"U_Country\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")  // write results to screen\n",
    "        .format(\"console\")\n",
    "        .start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@176c1d45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+---+\n",
      "|   name|city|country|age|\n",
      "+-------+----+-------+---+\n",
      "|Francis|null|     FR| 22|\n",
      "+-------+----+-------+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+-------------+-------+---+\n",
      "|  name|         city|country|age|\n",
      "+------+-------------+-------+---+\n",
      "|   Bob|     New York|     US| 22|\n",
      "|Denise|San Francisco|     US| 22|\n",
      "+------+-------------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(peopleStream.filter($\"age\" === 22)\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  // write results to screen\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Structured Stream with Datasets\n",
    "<!-- One of the best features of Structured Stream is the ability to natively join batch data with a Structured Stream. -->\n",
    "\n",
    "We can join datastreams with datasets.  Remember: both of these are distributed datasets and one is being streamed -- that's a lot of semantics for a simple `.join` operator!\n",
    "\n",
    "Below, we take a fixed user table and join it in with a stream of transactions in a fictitious poultry ecommerce website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class User\n",
       "defined class Transaction\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class User(id: Int, name: String, email: String, country: String)\n",
    "case class Transaction(userid: Int, product: String, cost: Double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+---------+\n",
      "|country|sum(cost)|\n",
      "+-------+---------+\n",
      "|     EN|     90.0|\n",
      "|     FR|     50.0|\n",
      "+-------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+---------+\n",
      "|country|sum(cost)|\n",
      "+-------+---------+\n",
      "|     EN|    180.0|\n",
      "|     FR|    100.0|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// A user dataset\n",
    "// Notice that we do not have to provide a schema\n",
    "// We can simply infer it\n",
    "val users = (spark.read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", true)\n",
    "    .csv(\"data/users.csv\")\n",
    "    .as[User]\n",
    ")\n",
    "\n",
    "val transactionSchema = (ScalaReflection\n",
    "    .schemaFor[Transaction]\n",
    "    .dataType\n",
    "    .asInstanceOf[StructType]\n",
    ")\n",
    "  \n",
    "// A stream of transactions\n",
    "val transactionStream = (spark.readStream\n",
    "    .schema(transactionSchema)\n",
    "    .option(\"header\", true)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"data/transactions/*.csv\")\n",
    "    .as[Transaction]\n",
    ")\n",
    "\n",
    "// Join transaction stream with user dataset\n",
    "val spendingByCountry = (transactionStream\n",
    "    .join(users, users(\"id\") === transactionStream(\"userid\"))\n",
    "    .groupBy($\"country\")\n",
    "    .agg(sum($\"cost\")) as \"spending\")\n",
    "    \n",
    "// Print result\n",
    "(spendingByCountry.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries in Spark Structured Streaming\n",
    "<!-- Spark also has an escape hatch into SQL queries that allows users to write familiar SQL queries against Structured Streams. -->\n",
    "\n",
    "Finally we can use the method `createOrReplaceTempView` to publish streams (and static datasets) as SQL tables.  We can then query the resulting table using SQL and stream the output as we would with any other datastream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Publish SQL table\n",
    "peopleStream.createOrReplaceTempView(\"peopleTable\")\n",
    "\n",
    "// SQL query\n",
    "val query = spark.sql(\"SELECT country, avg(age) FROM peopleTable GROUP BY country\")\n",
    "\n",
    "// Output\n",
    "(query.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregation in Structured Streaming\n",
    "<!-- We'll demonstrate how to perform groupBy and data aggregation in Structured Streaming.  We will also demonstrate how to use groupBy on multiple columns. -->\n",
    "\n",
    "We can use groupBy and aggregation as we would in SQL.\n",
    "\n",
    "- `groupBy` takes one or more `Column`s along which to groupBy.\n",
    "- The resulting object supports various built-in aggregation functions (`avg`, `mean`, `min`, `max`, `sum`) which take one or more string column names along which to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(peopleStream.groupBy('country)\n",
    "    .mean(\"age\")\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(peopleStream.groupBy('city)\n",
    "    .agg(first(\"country\") as \"country\", count(\"age\"))\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex aggregations, we can use `.agg`, which takes columns with aggregations.  Notice that we can reuse the keyword `as`, as well as other binary column operators from before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
